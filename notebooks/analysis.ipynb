{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T15:55:23.211375500Z",
     "start_time": "2023-10-17T15:55:21.279240100Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtensorboard\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SummaryWriter\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcudnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mcudnn\u001B[39;00m\n\u001B[0;32m     10\u001B[0m cudnn\u001B[38;5;241m.\u001B[39mbenchmark \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorboard\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdistutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LooseVersion\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(tensorboard, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__version__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m LooseVersion(\n\u001B[0;32m      5\u001B[0m     tensorboard\u001B[38;5;241m.\u001B[39m__version__\n\u001B[0;32m      6\u001B[0m ) \u001B[38;5;241m<\u001B[39m LooseVersion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.15\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "#  Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Python imports\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torchvision.models as tmodels\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os.path import join as ospj\n",
    "import itertools\n",
    "import glob\n",
    "import random\n",
    "\n",
    "#Local imports\n",
    "from data import dataset as dset\n",
    "from models.common import Evaluator\n",
    "from models.image_extractor import get_image_extractor\n",
    "from models.manifold_methods import RedWine, LabelEmbedPlus, AttributeOperator\n",
    "from models.modular_methods import GatedGeneralNN\n",
    "from models.symnet import Symnet\n",
    "from utils.utils import save_args, UnNormalizer, load_args\n",
    "from utils.config_model import configure_model\n",
    "from flags import parser\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run one of the cells to load the dataset you want to run test for and move to the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T15:44:52.375606300Z",
     "start_time": "2023-10-17T15:44:52.349855400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m best_mit \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../logs/graphembed/mitstates/base/mit.yml\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mload_args\u001B[49m(best_mit,args)\n\u001B[0;32m      3\u001B[0m args\u001B[38;5;241m.\u001B[39mgraph_init \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39margs\u001B[38;5;241m.\u001B[39mgraph_init\n\u001B[0;32m      4\u001B[0m args\u001B[38;5;241m.\u001B[39mload \u001B[38;5;241m=\u001B[39m best_mit[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m7\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mckpt_best_auc.t7\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'load_args' is not defined"
     ]
    }
   ],
   "source": [
    "best_mit = '../logs/graphembed/mitstates/base/mit.yml'\n",
    "load_args(best_mit,args)\n",
    "args.graph_init = '../'+args.graph_init\n",
    "args.load = best_mit[:-7] + 'ckpt_best_auc.t7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T15:44:52.847564500Z",
     "start_time": "2023-10-17T15:44:52.820938600Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m best_ut \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../logs/graphembed/utzappos/base/utzappos.yml\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mload_args\u001B[49m(best_ut,args)\n\u001B[0;32m      3\u001B[0m args\u001B[38;5;241m.\u001B[39mgraph_init \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39margs\u001B[38;5;241m.\u001B[39mgraph_init\n\u001B[0;32m      4\u001B[0m args\u001B[38;5;241m.\u001B[39mload \u001B[38;5;241m=\u001B[39m best_ut[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m12\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mckpt_best_auc.t7\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'load_args' is not defined"
     ]
    }
   ],
   "source": [
    "best_ut = '../logs/graphembed/utzappos/base/utzappos.yml'\n",
    "load_args(best_ut,args)\n",
    "args.graph_init = '../'+args.graph_init\n",
    "args.load = best_ut[:-12] + 'ckpt_best_auc.t7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading arguments and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.data_dir = '../'+args.data_dir\n",
    "args.test_set = 'test'\n",
    "testset = dset.CompositionDataset(\n",
    "        root= args.data_dir,\n",
    "        phase=args.test_set,\n",
    "        split=args.splitname,\n",
    "        model =args.image_extractor,\n",
    "        subset=args.subset,\n",
    "        return_images = True,\n",
    "        update_features = args.update_features,\n",
    "        clean_only = args.clean_only\n",
    "    )\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=args.test_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.workers)\n",
    "\n",
    "print('Objs ', len(testset.objs), ' Attrs ', len(testset.attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_extractor, model, optimizer = configure_model(args, testset)\n",
    "evaluator = Evaluator(testset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.load is not None:\n",
    "    checkpoint = torch.load(args.load)\n",
    "    if image_extractor:\n",
    "        try:\n",
    "            image_extractor.load_state_dict(checkpoint['image_extractor'])\n",
    "            image_extractor.eval()\n",
    "        except:\n",
    "            print('No Image extractor in checkpoint')\n",
    "    model.load_state_dict(checkpoint['net'])\n",
    "    model.eval()\n",
    "    print('Loaded model from ', args.load)\n",
    "    print('Best AUC: ', checkpoint['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(scores, exp):\n",
    "    print(exp)\n",
    "    result = scores[exp]\n",
    "    attr = [evaluator.dset.attrs[result[0][idx,a]] for a in range(topk)]\n",
    "    obj = [evaluator.dset.objs[result[1][idx,a]] for a in range(topk)]\n",
    "    attr_gt, obj_gt = evaluator.dset.attrs[data[1][idx]], evaluator.dset.objs[data[2][idx]]\n",
    "    print(f'Ground truth: {attr_gt} {obj_gt}')\n",
    "    prediction = ''\n",
    "    for a,o in zip(attr, obj):\n",
    "        prediction += a + ' ' + o + '| '\n",
    "    print('Predictions: ', prediction)\n",
    "    print('__'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of predictions\n",
    "closed -> Biased for unseen classes\n",
    "\n",
    "unbiiased -> Biased against unseen classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(testloader))\n",
    "images = data[-1]\n",
    "data = [d.to(device) for d in data[:-1]]\n",
    "if image_extractor:\n",
    "    data[0] = image_extractor(data[0])\n",
    "_, predictions = model(data)\n",
    "data = [d.to('cpu') for d in data]\n",
    "topk = 5\n",
    "results = evaluator.score_model(predictions, data[2], bias = 1000, topk=topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(images)):\n",
    "    seen = bool(evaluator.seen_mask[data[3][idx]])\n",
    "    if seen:\n",
    "        continue\n",
    "    image = Image.open(ospj( args.data_dir,'images', images[idx]))\n",
    "    \n",
    "    plt.figure(dpi=300)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'GT pair seen: {seen}')\n",
    "    print_results(results, 'closed')\n",
    "    print_results(results, 'unbiased_closed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "args.bias = 1e3\n",
    "accuracies, all_attr_gt, all_obj_gt, all_pair_gt, all_pred = [], [], [], [], []\n",
    "\n",
    "for idx, data in tqdm(enumerate(testloader), total=len(testloader), desc = 'Testing'):\n",
    "    data.pop()\n",
    "    data = [d.to(device) for d in data]\n",
    "    if image_extractor:\n",
    "        data[0] = image_extractor(data[0])\n",
    "\n",
    "    _, predictions = model(data) # todo: Unify outputs across models\n",
    "\n",
    "    attr_truth, obj_truth, pair_truth = data[1], data[2], data[3]\n",
    "    all_pred.append(predictions)\n",
    "    all_attr_gt.append(attr_truth)\n",
    "    all_obj_gt.append(obj_truth)\n",
    "    all_pair_gt.append(pair_truth)\n",
    "\n",
    "all_attr_gt, all_obj_gt, all_pair_gt = torch.cat(all_attr_gt), torch.cat(all_obj_gt), torch.cat(all_pair_gt)\n",
    "\n",
    "all_pred_dict = {}\n",
    "# Gather values as dict of (attr, obj) as key and list of predictions as values\n",
    "for k in all_pred[0].keys():\n",
    "    all_pred_dict[k] = torch.cat(\n",
    "        [all_pred[i][k] for i in range(len(all_pred))])\n",
    "\n",
    "# Calculate best unseen accuracy\n",
    "attr_truth, obj_truth = all_attr_gt.to('cpu'), all_obj_gt.to('cpu')\n",
    "pairs = list(\n",
    "    zip(list(attr_truth.numpy()), list(obj_truth.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 1 ### For topk results\n",
    "our_results = evaluator.score_model(all_pred_dict, all_obj_gt, bias = 1e3, topk = topk)\n",
    "stats = evaluator.evaluate_predictions(our_results, all_attr_gt, all_obj_gt, all_pair_gt, all_pred_dict, topk = topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in stats.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "test",
   "language": "python",
   "display_name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
